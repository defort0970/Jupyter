{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bookscraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1v6AwFjpHXlAVvRBGFRCiqXEgVpuk9b0_",
      "authorship_tag": "ABX9TyOxu3szFk8CW3IICFvb/xY4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/defort0970/Jupyter/blob/master/bookscraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mte7EwqTCN4s",
        "outputId": "6b1560d0-3463-4b08-d35a-f057d3f57771"
      },
      "source": [
        "!pip install --upgrade woocommerce\n",
        "!pip install googletrans==3.1.0a0\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import gspread\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive/')\n",
        "sys.path.append('/content/drive/MyDrive/ColabNotebooks/')\n",
        "\n",
        "from jcmodule import *\n",
        "from const import *\n",
        "\n",
        "wcapi=getjcapi(JCAPIURL,JCCK1,JCCS1,JCTMO180)\n",
        "\n",
        "SERVICE_ACCOUNT_FILE = '/content/drive/MyDrive/ColabNotebooks/emtional-healingart-61b6632f5288.json'\n",
        "scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
        "credentials = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_FILE, scope) # Your json file here\n",
        "gc = gspread.authorize(credentials)\n",
        "sheet = gc.open(\"左西購物網產品上架規格\").sheet1\n",
        "\n",
        "data = sheet.get_all_values()\n",
        "headers = data.pop(0)\n",
        "\n",
        "df = pd.DataFrame(data, columns=headers)\n",
        "max_rows = len(sheet.get_all_values())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: woocommerce in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from woocommerce) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->woocommerce) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->woocommerce) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->woocommerce) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->woocommerce) (3.0.4)\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.22)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.4.0)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-0rvJXOu5Ys"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "def finddetail(words,reptext):\n",
        "    df = []\n",
        "    for item in words:\n",
        "        item = item.text.replace(' ','')\n",
        "        df.append(item)\n",
        "    df = pd.DataFrame(df)\n",
        "    newstr = df[df[0].str.contains(reptext)]\n",
        "    if newstr.empty:\n",
        "        return \"\"\n",
        "    else:\n",
        "        nwdf = str(newstr).replace('\\n','').strip()\n",
        "    return ''.join(nwdf)[4:].strip()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUerY7TCSdwY"
      },
      "source": [
        "def fetchbooks(indno):\n",
        "    idx = indno-2\n",
        "    url = df.loc[idx][2].split('?')[0]\n",
        "    urlforimg = url.split('/')[-1]\n",
        "    html = requests.get(url).text\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    images = soup.findAll('img')\n",
        "    catalogno = df.loc[idx][5]\n",
        "\n",
        "    bookprice = soup.find('div', { 'class': 'cnt_prod002 clearfix' }).text.split('\\n')\n",
        "    bookinfo = soup.find('div', { 'class': 'type02_p003 clearfix' }).text.split('\\n')\n",
        "    bkki = []\n",
        "    bkki = bookinfo[-3].replace(' ','')\\\n",
        "    +bookinfo[-2].replace('原文作者','\\n原文作者').replace('新功能介紹','').replace('譯者','\\n譯者').replace('出版社：','\\n出版社：').replace(' ','')\\\n",
        "    +bookinfo[-1].replace('新功能介紹','').replace('語言','\\n語言').replace('出版日期','\\n出版日期')\n",
        "\n",
        "    bkprice = bookprice[4].replace(\"定價：\",\"\").replace(\"原價：\",\"\").replace(\"元\",\"\")\n",
        "    sheet.update_acell('M'+str(indno), bkprice)\n",
        "    sheet.update_acell('N'+str(indno), bkki)\n",
        "\n",
        "\n",
        "    if len(soup.find_all('div', { 'class': 'mod type02_p002 clearfix' }))>0:#書籍類別\n",
        "        for ps in soup.find_all('div', { 'class': 'mod type02_p002 clearfix' }):\n",
        "            tile = ps.select('h1')[0].text.split('\\n')\n",
        "            cctile = tile[0].replace(':','').replace('?','')\n",
        "            slugglong = [sub for sub in jtranslate(cctile,'en') if sub.isalpha() or sub.isspace()]\n",
        "            slugglong = ''.join(slugglong)\n",
        "            slugg ='-'.join(slugglong.split()[:6])\n",
        "            slugg =slugg[:16].replace(':','').replace('?','').replace('--','')\n",
        "    elif len(soup.find_all('div', { 'class': 'cnt_prod002 clearfix' }))>0:#桌遊類別\n",
        "        for ps in soup.find_all('div', { 'class': 'mod prd001' }):\n",
        "            tile = ps.select('h1')[0].text.split('\\n')\n",
        "            cctile = tile[0].replace(':','').replace('?','')\n",
        "            slugglong = [sub for sub in jtranslate(cctile,'en') if sub.isalpha() or sub.isspace()]\n",
        "            slugglong = ''.join(slugglong)\n",
        "            slugg ='-'.join(slugglong.split()[:6])\n",
        "            slugg =slugg[:16].replace(':','').replace('?','').replace('--','')\n",
        "\n",
        "    sheet.update_acell('V'+str(indno),str(slugg))#Slug 英文名稱\n",
        "    coverimg = \"https://www.books.com.tw/img/\"+str(urlforimg[:3])+'/'+str(urlforimg[3:6])+'/'+str(urlforimg[6:8])+'/'+str(urlforimg)+\".jpg\"\n",
        "    sheet.update_acell('W'+str(indno),coverimg)#coverimg\n",
        "\n",
        "    ix = []\n",
        "    ixt = []\n",
        "\n",
        "    without_duplicates = []\n",
        "    for image in images:\n",
        "        imgurl = image['src']\n",
        "        if imgurl[:70] == \"https://im2.book.com.tw/image/getImage?i=https://www.books.com.tw/img/\" or\\\n",
        "            imgurl[:70] == \"https://im1.book.com.tw/image/getImage?i=https://www.books.com.tw/img/\" :\n",
        "            if imgurl[81:91] == urlforimg:#[10]:\n",
        "                a1image = imgurl[:91]+\".jpg&w=500&h=600\"\n",
        "                andpostion = imgurl.find('&')\n",
        "                ix = imgurl[41:andpostion]\n",
        "                ixt.extend([ix])\n",
        "                [without_duplicates.append(element) for element in ixt if element not in without_duplicates]\n",
        "            nodimages = ','.join(without_duplicates[1:])\n",
        "\n",
        "    for detailinfo in soup.find_all('div', { 'class': 'mod_b type02_m058 clearfix' }):\n",
        "        checkitem =['ISBN：','叢書系列：','規格：','出版地：','本書分類：']\n",
        "        infolist = [finddetail(detailinfo.select('li'),checkitem[i]).replace(checkitem[i],'') for i in range(5)]\n",
        "        sheet.update_acell('L'+str(indno),infolist[0])#ISBN Number\n",
        "        sheet.update_acell('O'+str(indno),infolist[1])#叢書系列\n",
        "        sheet.update_acell('P'+str(indno),infolist[2])#規格\n",
        "        sheet.update_acell('Q'+str(indno),infolist[3])#出版地\n",
        "        sheet.update_acell('R'+str(indno),infolist[4].replace(' ',''))#分類\n",
        "        dimsn = str(infolist[2]).replace('cm','').split('/')[2] #新版規格\n",
        "        isbnno = infolist[0]\n",
        "        try:\n",
        "            length = dimsn.split('x')[0]\n",
        "            width = dimsn.split('x')[1]\n",
        "            if len(dimsn.split('x'))==2:\n",
        "                height = ''\n",
        "            else:\n",
        "                height = dimsn.split('x')[2]\n",
        "        except IndexError as e:\n",
        "            length = \"\"\n",
        "            width = \"\"\n",
        "            height = \"\"\n",
        "\n",
        "    sheet.update_acell('S'+str(indno),length)#尺寸：長\n",
        "    sheet.update_acell('T'+str(indno),width)#尺寸：寬\n",
        "    sheet.update_acell('U'+str(indno),height)#尺寸：高\n",
        "  \n",
        "    #封面圖先上傳 GD 並且轉換尺寸，拋轉到 Cloud Storage，再轉入 Woocommerce\n",
        "    drfilepath = \"/content/drive/MyDrive/JCBimages/\"\n",
        "    imgrzpath = str(isbnno)+slugg+\"-rzcvr\"+\".jpg\"\n",
        "    firstfilename = drfilepath+str(isbnno)+slugg+\".jpg\"\n",
        "    cvfilename = drfilepath+imgrzpath\n",
        "\n",
        "    try:\n",
        "        urllib.request.urlretrieve(coverimg[:50]+\".jpg\",firstfilename)\n",
        "        imgresizefun(firstfilename,cvfilename,JCCOVER_H,JCCOVER_W)\n",
        "    except urllib.error.HTTPError as e:\n",
        "        pass\n",
        "\n",
        "    upload_to_bucket(imgrzpath,cvfilename,\"staging.emtional-healingart.appspot.com\")\n",
        "    cvig = \"https://storage.googleapis.com/staging.emtional-healingart.appspot.com/\"+imgrzpath\n",
        "    allimages = cvig+','+nodimages\n",
        "    print(allimages)\n",
        "\n",
        "    accordion = ['[/accordion-item]','[accordion-item title=\"作者介紹\"]','[accordion-item title=\"目錄\"]','[accordion-item title=\"序言\"]','[accordion-item title=\"內容連載\"]']\n",
        "    descontanct =[]\n",
        "    for ct,i in zip(soup.find_all('div', { 'class': 'mod_b type02_m057 clearfix' }),range(4)):\n",
        "        ct = ct.select('div')[0]#.replace('\\xa0','').replace('\\n\\n','').replace('\\u3000','').replace('\\r','')#.split('\\n')\n",
        "        if i==0:\n",
        "            descontanct.append(ct)\n",
        "        elif i == 1:\n",
        "            descontanct.append(accordion[1])\n",
        "            descontanct.append(ct)\n",
        "            descontanct.append(accordion[0])\n",
        "        elif i == 2:\n",
        "            descontanct.append(accordion[2])\n",
        "            descontanct.append(ct)\n",
        "            descontanct.append(accordion[0])\n",
        "        elif i == 3:\n",
        "            descontanct.append(accordion[3])\n",
        "            descontanct.append(ct)\n",
        "            descontanct.append(accordion[0])\n",
        "    # print(descontanct)\n",
        "    descontanct.append(accordion[4])\n",
        "    for ctseon in soup.find_all('div', { 'class': 'mod_b type02_m059 clearfix' }):\n",
        "        ctseon = ctseon.select('div')[0]\n",
        "        descontanct.append(ctseon)\n",
        "\n",
        "    urlcontant = url.split('/')\n",
        "    urlbook = \"https://www.books.com.tw/web/sys_serialtext/?item=\"+str(urlcontant[-1])#str(urlcontant[10])\n",
        "    for i in range(2,12):\n",
        "        urlpage = urlbook+\"&page=\"+str(i)\n",
        "        urlgo = requests.get(urlpage).text\n",
        "        soup = BeautifulSoup(urlgo, \"lxml\")\n",
        "        time.sleep(0.1*i)\n",
        "        if soup:\n",
        "            for cttn in soup.find_all('div', { 'class': 'mod type02_specific_02 clearfix' }):\n",
        "                cttn = cttn.select('div')[1]\n",
        "                descontanct.append(cttn)\n",
        "    descontanct.append(accordion[0])\n",
        "      \n",
        "    toremove = [\n",
        "        '<div>','</div>','<div class=\"type02_gradient\" style=\"display:none;\">',\\\n",
        "        '<div class=\"cont\">','<div style=\"text-align: center\">','<div class=\"bd\">',\\\n",
        "        '</div><div class=\"type02_gradient\" style=\"display:none;',\\\n",
        "        '<div class=\"content\" style=\"height:auto;\">','\"></div>','</div>',\\\n",
        "        '</div><div class=\"type02_gradient\" id=\"M201105_0_getProdTextInfo_P00a400020009_g3\" style=\"display:block;','</div>]',\\\n",
        "        '</div>,','</div><div class=\"type02_gradient\" id=\"M201105_0_getProdTextInfo_P00a400020009_g2\" style=\"display:block;\">',\\\n",
        "        '<div class=\"content more_off\" id=\"M201105_0_getProdTextInfo_P00a400020009_h3\">',\\\n",
        "        '<div class=\"content more_off\" id=\"M201105_0_getProdTextInfo_P00a400020009_h2\">','<div style=\"text-align: center;\">',\\\n",
        "        '<div class=\"type02_gradient\" id=\"M201105_0_getProdTextInfo_P00a400020009_g2\" style=\"display:block;\">',\\\n",
        "        '<div style=\"text-align: right;\">','<div class=\"type02_gradient\" id=\"M201105_0_getProdTextInfo_P00a400020009_g3\" style=\"display:block;\">',\\\n",
        "        ', ','<div class=\"content\">','<div class=\"type02_gradient\" style=\"display:block;','<div class=\"hr_shadow clearfix\">',\\\n",
        "        '<b class=\"bl\"></b>','　　']\n",
        "\n",
        "    for tm in toremove:\n",
        "        descontanct = str(descontanct).replace(tm,\"\")\n",
        "        descontanct = str(descontanct).replace(\"'\",\"\")\n",
        "        descontanct= descontanct[1:-1]\n",
        "        sheet.update_acell('AA'+str(indno),descontanct)#內文\n",
        "        \n",
        "    dfnew = allimages.split(',')\n",
        "    image_data = []\n",
        "    dfnew = allimages.split(',')\n",
        "    for k in range(len(dfnew)):\n",
        "        if dfnew[k]:\n",
        "            image_data.append({ \"src\": dfnew[k],\n",
        "                                \"position\": k,\n",
        "                                \"name\": str(cctile[:10])+'-'+str(k),\n",
        "                                \"alt\": str(slugg)+'-'+str(k),})\n",
        "        \n",
        "    data = {\"name\": str(cctile),\n",
        "            \"slug\": str(slugg),\n",
        "            \"type\": \"simple\",\n",
        "            \"sku\": str(isbnno),\n",
        "            \"short_description\": str(bkki),\n",
        "            \"description\":str(descontanct),\n",
        "            \"regular_price\":str(bkprice),\n",
        "            \"categories\": [{'id': str(catalogno)}],\n",
        "            \"manage_stock\": True,\n",
        "            \"stock_quantity\": 0,\n",
        "            \"stock_status\": \"outofstock\",\n",
        "            \"weight\": \"\",\n",
        "            \"dimensions\": { \"length\":str(length),\n",
        "                            \"width\": str(width),\n",
        "                            \"height\": str(height)},\n",
        "            \"images\": image_data}\n",
        "    try:\n",
        "        wcapi.post(\"products/\", data).json()\n",
        "    except JSONDecodeError as e:\n",
        "        print('Fail in POST Jason')\n",
        "        pass\n",
        "\n",
        "    print(\"Finished!!\")\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s76vCziVveZA"
      },
      "source": [
        "def jctages():\n",
        "    Flag = True\n",
        "    page =1 \n",
        "    data = []\n",
        "    dftags = pd.DataFrame()\n",
        "    while Flag:\n",
        "        tagdata = wcapi.get(\"products/tags/?page=\"+str(page)).json()\n",
        "        data = pd.json_normalize(tagdata)\n",
        "        if len(tagdata) == 0: # no more tags\n",
        "            Flag = False\n",
        "            break\n",
        "        page += 1\n",
        "        dftags = dftags.append(data)\n",
        "    \n",
        "    dftags = dftags.reset_index(drop=True)\n",
        "    return dftags"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rFkeWmxvySR"
      },
      "source": [
        "def alltagsupdate(dfbuy,dbtags,dbdesn,dbsku,wcapi):\n",
        "    dftags = jctages()\n",
        "    dbtags = dbtags.fillna(\"\")\n",
        "    dftags['name'] = dftags['name'].astype(str)\n",
        "\n",
        "    for i in range(len(dbtags)):\n",
        "        if not dbtags.loc[i]:\n",
        "            dbtags.loc[i] =\"\"\n",
        "        else:\n",
        "            dbtags.loc[i] = ','.join(dbtags.loc[i].split('|'))\n",
        "\n",
        "    dbdesn = dbdesn.astype(str)\n",
        "    temp = []\n",
        "    for i in range(len(dbtags)):\n",
        "        for k in dbtags.loc[i].split(','):\n",
        "            for j in range(len(dftags['name'])):\n",
        "                if k.count(dftags['name'][j]):\n",
        "                    temp.append(str(dftags['id'].loc[j]))\n",
        "        dbtags.loc[i] = ','.join(temp)\n",
        "        temp = []\n",
        "\n",
        "    for i in range(len(dbdesn)):\n",
        "        for j in range(len(dftags['name'])):\n",
        "            if dbdesn[i].count(dftags['name'][j]):\n",
        "                dbtags.loc[i] = dbtags.loc[i]+','+str(dftags['id'].loc[j])\n",
        "                dbtags.loc[i] = ','.join(list(dict.fromkeys(dbtags.loc[i].split(','))))\n",
        "        if not len(dbtags[i]):\n",
        "            dbtags[i] = \",\"\n",
        "        if dbtags[i][0] == ',':\n",
        "            dbtags[i]=dbtags[i][1:]\n",
        "\n",
        "    for i,ptno,tagname in zip(range(len(dfbuy)),dbsku,dbtags):\n",
        "        productlist = False\n",
        "        while not productlist:\n",
        "            productlist = wcapi.get(\"products/?sku=\"+str(ptno)).json()\n",
        "            time.sleep(22)\n",
        "            print(\"wait for updates\")\n",
        "\n",
        "        productid = productlist[0]['id']\n",
        "        dfnew=dbtags[i].split(',')\n",
        "        data=[]\n",
        "        for k in range(len(dfnew)):\n",
        "            data.append({\"id\": dfnew[k]},)\n",
        "        data = {\"tags\":data,\n",
        "                \"status\": \"draft\",}\n",
        "        \n",
        "        try:\n",
        "            wcapi.put(\"products/\"+str(productid), data).json()\n",
        "            print(productid,data)\n",
        "        except JSONDecodeError as e:\n",
        "            print('Fail in POST Jason')\n",
        "            pass"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxxWo92uRe-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "204ce195-4f0e-4278-d33f-3b8cf8db9205"
      },
      "source": [
        "for i in range(int(max_rows),0,-1):\n",
        "    if sheet.acell('A'+str(i)).value == \"TRUE\":\n",
        "        print(sheet.acell('B'+str(i)).value)\n",
        "        fetchbooks(i)\n",
        "        ironmen_dict = {\"Tag\":\"\",\"Description\": str(sheet.acell('AA'+str(i)).value),\"Product SKU\": str(sheet.acell('L'+str(i)).value)}\n",
        "        dfbuy = pd.DataFrame(ironmen_dict,index=[0])\n",
        "        alltagsupdate(dfbuy,dfbuy['Tag'],dfbuy['Description'],dfbuy['Product SKU'],wcapi)\n",
        "        print('Tags update OK.')\n",
        "        sheet.update_acell('A'+str(i),\"FALSE\")\n",
        "        sheet.update_acell('H'+str(i),\"TRUE\")\n",
        "        time.sleep(0.2)\n",
        "    else:\n",
        "        time.sleep(i/100)\n",
        "        print('Loop:',i)\n",
        "        pass\n",
        "\n",
        "print('作業完成，圓滿結束！')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "海奧華預言：第九級星球的九日旅程‧奇幻不思議的真實見聞\n",
            "1.285 26 0 26 800 986\n",
            "https://storage.googleapis.com/staging.emtional-healingart.appspot.com/9789869854825Theoward-Prophec-rzcvr.jpg,https://www.books.com.tw/img/001/084/60/0010846057_bc_01.jpg\n",
            "Finished!!\n",
            "wait for updates\n",
            "330043 {'tags': [{'id': '1140'}, {'id': '1100'}, {'id': '1081'}, {'id': '1230'}, {'id': '1025'}, {'id': '1187'}, {'id': '1237'}, {'id': '1199'}, {'id': '1227'}, {'id': '1203'}, {'id': '1235'}, {'id': '1229'}, {'id': '1190'}, {'id': '1101'}, {'id': '1234'}, {'id': '1051'}, {'id': '1244'}, {'id': '891'}, {'id': '1209'}, {'id': '1145'}], 'status': 'draft'}\n",
            "Tags update OK.\n",
            "Loop: 8\n",
            "Loop: 7\n",
            "Loop: 6\n",
            "Loop: 5\n",
            "Loop: 4\n",
            "Loop: 3\n",
            "Loop: 2\n",
            "Loop: 1\n",
            "作業完成，圓滿結束！\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}